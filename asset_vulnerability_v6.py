import warnings
import pandas as pd
import geopandas as geo
import numpy as np
from tqdm import tqdm
import os
import plotly.express as px
import uuid
import osmnx as ox
from keplergl import KeplerGl
from shapely.geometry import Point, Polygon, MultiPolygon, MultiPolygon, MultiPoint
from geopy.geocoders import Nominatim
locator = Nominatim(user_agent="myGeocoder")
import requests

ox.settings.log_console=True
warnings.simplefilter(action='ignore', category=FutureWarning)

#osmnx config timeout
timeout = 3000
ox.config(timeout=timeout)

### import tables

nws_region = pd.read_csv('data_dir/nws_region_codes.csv')
state_ab = pd.read_csv('data_dir/state_abbv.csv')
counties = geo.read_file('data_dir/c_13se22/c_13se22.shp')

#### create dictionary from NWS alerts, add geocodes ####

w_d = {'state': [], 'area': [], 'event':[], 'sent':[], 'onset':[], 'ends':[], 'geocode': [], 'severity': [],
       'certainty':[], 'urgency':[], 'headline': [], 'descr': []}

for ab in state_ab.state_abbv:
    url = f'https://api.weather.gov/alerts/active?area={ab}'
    response = requests.get(url)
    if response.status_code == 200:
        data = response.json()
    else:
        print(f'Error: {response.status_code}')
       
    for i in data['features']:
        #print(i['properties'])
        if i['properties']['severity'] == 'Severe':
            state = ab
            w_d['state'].append(state)
            area = i['properties']['areaDesc']
            w_d['area'].append(area)
            event = i['properties']['event']
            w_d['event'].append(event)
            sent = i['properties']['sent']
            w_d['sent'].append(sent)
            onset = i['properties']['onset']
            w_d['onset'].append(onset)
            ends = i['properties']['ends']
            w_d['ends'].append(ends)
            geocode = i['properties']['geocode']['SAME']
            w_d['geocode'].append(geocode)
            severity = i['properties']['severity']
            w_d['severity'].append(severity)
            urgency = i['properties']['urgency']
            w_d['urgency'].append(urgency)
            certainty = i['properties']['certainty']
            w_d['certainty'].append(certainty)
            headline = i['properties']['headline']
            w_d['headline'].append(headline)
            descr = i['properties']['description']
            w_d['descr'].append(descr)
            
report = pd.DataFrame(dict([ (k,pd.Series(v)) for k,v in w_d.items() ]))
report['sent'] = pd.to_datetime(report['sent'], utc = True)
report['onset'] = pd.to_datetime(report['onset'], utc = True)
report['ends'] = pd.to_datetime(report['ends'], utc = True)
report.to_csv('report.csv')

#take all geocodes associated with report, and match then to FIPS geocodes from Counties shp

affected = []

for i in report['geocode'].iloc[0:]:
    no_zeros = [zero.lstrip('0') for zero in i]
    for nzero in no_zeros:
        affected.append(nzero)
        
affected_nd = list(set(affected))
affected_nd.sort(reverse = False)


affected_counties = geo.GeoDataFrame()

try:
    for f in affected_nd:
        counties['FIPS'] = counties['FIPS'].str.lstrip('0')
        county = counties[(counties['FIPS'] == f)]
        county = county.to_crs(4326)
        affected_counties = pd.concat([affected_counties, county])
except FutureWarning:
    pass

### prep the data, perform a table merge on counties and report

report = report.explode('geocode')
report = report.drop_duplicates(subset = ['geocode', 'event', 'area'])
report['geocode'] = report['geocode'].str.lstrip('0')
merged_affected = pd.merge(report, affected_counties, left_on='geocode', right_on='FIPS', how='left')
geo_merge = geo.GeoDataFrame(merged_affected, geometry='geometry')
geo_merge.to_csv('geo_merge.csv')
geo_merge = geo_merge.to_crs(4326)

### using the unique geocodes from the report, pull osm geometries (power : plant) ###

affected_utilities = geo.GeoDataFrame()

for code in geo_merge['geocode'].unique():
    geo_u = geo_merge[(geo_merge.geocode == code)]
    geom = geo_u.geometry.iloc[0]
    tags = {'power': 'plant'}        
    osm_pull = ox.geometries.geometries_from_polygon(geom, tags)
    osm_pull.reset_index(inplace = True)
    affected_utilities = pd.concat([affected_utilities, osm_pull])
    affected_utilities.to_csv('affected_utilities.csv')

### select only a few rows

v_utils = affected_utilities[['geometry', 'name', 'operator', 'plant:source', 'plant:method',
                              'plant:output:electricity',  'source', 'osmid', 'element_type', 'power']]

### perform a spatial merge on the returned osm_boundaries and the merged file, delete dupes

prod = geo.GeoDataFrame(v_utils, geometry='geometry')
merged = geo.sjoin(prod, geo_merge, how='inner', op='intersects')
final = merged.drop_duplicates(subset = 'osmid')    # MAY NOT HAVE WANTED TO DO THIS ??
final['centroid'] = final['geometry'].centroid
final['descr'] = final['descr'].str.wrap(500)
final = final.fillna('NA')

### more cleaning
final['certainty_n'] = ''
final['urgency_n'] = ''
final['certainty_n'] = final['certainty'].apply(lambda x: 3 if x == 'Observed' else (2 if x == 'Likely' else (1 if x == 'Possible' else None)))
final['urgency_n'] = final['urgency'].apply(lambda x: 3 if x == 'Immediate' else (2 if x == 'Expected' else (1 if x == 'Future' else 0)))
final.to_csv('final.csv')
final.head()

### Current Threats, Expected Threats, Future Threats, 1, 2, 3
#current = final[(final.certainty == 'Observed') | (final.certainty == 'Likely') & (final.urgency == 'Immediate')]
#current = final[(final.certainty == 'Observed' | final.certainty == 'Likely') & (final.urgency == 'Immediate')]
current = final[(final.urgency == 'Immediate')]
expected = final[(final.certainty == 'Likely') & (final.urgency == 'Expected')]
future = final[(final.certainty == 'Possible') & (final.urgency == 'Future')]


## MORE GRANULAR
## getting forcast for each grid associated with 'query' level
## NEXT LEVEL BABYYYYYYY ~ something I have yet to do in python (list to columns in a df)

current_df = geo.GeoDataFrame() #final geodataframe
current_d_forecast = []
current_s_forecast = []
current_temp = []
current_w_speed = []
current_chance_of_precip = []
current_elev = []

for x, y in tqdm(zip(current['centroid'].x, current['centroid'].y)):
    url = f'https://api.weather.gov/points/{y},{x}'
    response = requests.get(url)
    if response.status_code == 200:
        data = response.json()
        forecast = data['properties']['forecast']
    else:
        print(f'Error: {response.status_code}')
    url_forecast = forecast
    response = requests.get(url_forecast)
    if response.status_code == 200:
        data_forecast = response.json()
        elevation = data_forecast['properties']['elevation']['value']
        detailed_forecast = data_forecast['properties']['periods'][0]['detailedForecast']
        short_forecast = data_forecast['properties']['periods'][0]['shortForecast']
        temperature = data_forecast['properties']['periods'][0]['temperature']
        wind_speed = data_forecast['properties']['periods'][0]['windSpeed'][:-4]
        per_precip = data_forecast['properties']['periods'][0]['probabilityOfPrecipitation']['value']
        current_d_forecast.append(detailed_forecast)
        current_s_forecast.append(short_forecast)
        current_temp.append(temperature)
        current_w_speed.append(wind_speed)
        current_chance_of_precip.append(per_precip)
        current_elev.append(elevation)
        geojson = data_forecast['geometry']['coordinates']
        polygon = Polygon(geojson[0])
        gdf = geo.GeoDataFrame(geometry=[polygon], crs='EPSG:4326')
        #current_df = current_df.append(gdf)
        current_df = pd.concat([current_df, gdf])

### adding lists above to df, data cleaning

current_df = current_df.to_crs(4326)
current_df['centroid'] = current_df['geometry'].centroid
current_df['detailed_forecast'] = current_d_forecast
current_df['short_forecast'] = current_s_forecast
current_df['temperature'] = current_temp
current_df['wind_speed'] = current_w_speed
current_df['chance_of_precipitation'] = current_chance_of_precip
current_df['elevation_m'] = current_elev
current_df['elevation_ft'] = current_df['elevation_m'] * 3.28084
current_df.fillna(0, inplace = True)
current_df = current_df.to_crs(4326)
current_df.to_csv('current_df.csv')

### spatial_join the f_df grid data on the osmnx query

current.drop(columns = 'index_right', inplace = True)
join_current = current.sjoin_nearest(current_df, how='inner', lsuffix='left', rsuffix='right')
join_current.sort_values(by = 'name', inplace = True)
join_current.drop_duplicates(subset = ['name', 'osmid'], inplace = True) #### !!!!!!!!! maybe also implement above
join_current['centroid'] = join_current['geometry'].centroid

### expected 
'''
expected_df = geo.GeoDataFrame() #final geodataframe
expected_d_forecast = []
expected_s_forecast = []
expected_temp = []
expected_w_speed = []
expected_chance_of_precip = []
expected_elev = []

for x, y in tqdm(zip(expected['centroid'].x, expected['centroid'].y)):
    url = f'https://api.weather.gov/points/{y},{x}'
    response = requests.get(url)
    if response.status_code == 200:
        data = response.json()
        forecast = data['properties']['forecast']
    else:
        print(f'Error: {response.status_code}')
    url_forecast = forecast
    response = requests.get(url_forecast)
    if response.status_code == 200:
        data_forecast = response.json()
        elevation = data_forecast['properties']['elevation']['value']
        detailed_forecast = data_forecast['properties']['periods'][0]['detailedForecast']
        short_forecast = data_forecast['properties']['periods'][0]['shortForecast']
        temperature = data_forecast['properties']['periods'][0]['temperature']
        wind_speed = data_forecast['properties']['periods'][0]['windSpeed'][:-4]
        per_precip = data_forecast['properties']['periods'][0]['probabilityOfPrecipitation']['value']
        expected_d_forecast.append(detailed_forecast)
        expected_s_forecast.append(short_forecast)
        expected_temp.append(temperature)
        expected_w_speed.append(wind_speed)
        expected_chance_of_precip.append(per_precip)
        expected_elev.append(elevation)
        geojson = data_forecast['geometry']['coordinates']
        polygon = Polygon(geojson[0])
        gdf = geo.GeoDataFrame(geometry=[polygon], crs='EPSG:4326')
        #expected_df = expected_df.append(gdf)
        expected_df = pd.concat([expected_df, gdf])

### adding lists above to df, data cleaning

expected_df['centroid'] = expected_df['geometry'].centroid
expected_df['detailed_forecast'] = expected_d_forecast
expected_df['short_forecast'] = expected_s_forecast
expected_df['temperature'] = expected_temp
expected_df['wind_speed'] = expected_w_speed
expected_df['chance_of_precipitation'] = expected_chance_of_precip
expected_df['elevation_m'] = expected_elev
expected_df['elevation_ft'] = expected_df['elevation_m'] * 3.28084
expected_df.fillna(0, inplace = True)
expected_df = expected_df.to_crs(4326)
expected_df.to_csv('expected_df.csv')

### spatial_join the f_df grid data on the osmnx query

expected.drop(columns = 'index_right', inplace = True)
join_expected = expected.sjoin_nearest(expected_df, how='inner', lsuffix='left', rsuffix='right')
join_expected.sort_values(by = 'name', inplace = True)
join_expected.drop_duplicates(subset = ['name', 'osmid'], inplace = True) #### !!!!!!!!! maybe also implement above

### future 

future_df = geo.GeoDataFrame() #final geodataframe
future_d_forecast = []
future_s_forecast = []
future_temp = []
future_w_speed = []
future_chance_of_precip = []
future_elev = []

for x, y in tqdm(zip(future['centroid'].x, future['centroid'].y)):
    url = f'https://api.weather.gov/points/{y},{x}'
    response = requests.get(url)
    if response.status_code == 200:
        data = response.json()
        forecast = data['properties']['forecast']
    else:
        print(f'Error: {response.status_code}')
    url_forecast = forecast
    response = requests.get(url_forecast)
    if response.status_code == 200:
        data_forecast = response.json()
        elevation = data_forecast['properties']['elevation']['value']
        detailed_forecast = data_forecast['properties']['periods'][0]['detailedForecast']
        short_forecast = data_forecast['properties']['periods'][0]['shortForecast']
        temperature = data_forecast['properties']['periods'][0]['temperature']
        wind_speed = data_forecast['properties']['periods'][0]['windSpeed'][:-4]
        per_precip = data_forecast['properties']['periods'][0]['probabilityOfPrecipitation']['value']
        future_d_forecast.append(detailed_forecast)
        future_s_forecast.append(short_forecast)
        future_temp.append(temperature)
        future_w_speed.append(wind_speed)
        future_chance_of_precip.append(per_precip)
        future_elev.append(elevation)
        geojson = data_forecast['geometry']['coordinates']
        polygon = Polygon(geojson[0])
        gdf = geo.GeoDataFrame(geometry=[polygon], crs='EPSG:4326')
        #future_df = future_df.append(gdf)
        future_df = pd.concat([future_df, gdf])

### adding lists above to df, data cleaning

future_df['centroid'] = future_df['geometry'].centroid
future_df['detailed_forecast'] = future_d_forecast
future_df['short_forecast'] = future_s_forecast
future_df['temperature'] = future_temp
future_df['wind_speed'] = future_w_speed
future_df['chance_of_precipitation'] = future_chance_of_precip
future_df['elevation_m'] = future_elev
future_df['elevation_ft'] = future_df['elevation_m'] * 3.28084
future_df.fillna(0, inplace = True)
future_df = future_df.to_crs(4326)
future_df.to_csv('future_df.csv')

### spatial_join the f_df grid data on the osmnx query

future.drop(columns = 'index_right', inplace = True)
join_future = future.sjoin_nearest(future_df, how='inner', lsuffix='left', rsuffix='right')
join_future.sort_values(by = 'name', inplace = True)
join_future.drop_duplicates(subset = ['name', 'osmid'], inplace = True) #### !!!!!!!!! maybe also implement above
'''
print('COMPLETE')
